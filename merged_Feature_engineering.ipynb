{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Merged Feature Engineering\n",
    "\n",
    "## Task Checklist — ALL FEATURE ENGINEERING COMPLETE\n",
    "\n",
    "1. **Aggregate depth=1 and depth=2 tables** - DONE\n",
    "2. **Split data into Applicant vs others (num_group1)** - DONE (person_1)\n",
    "3. **Active and closed contracts** - DONE\n",
    "4. **Time-windowed aggregations** - DONE\n",
    "5. **DPD-conditional aggregations** - DONE\n",
    "6. **Static table features** - DONE\n",
    "7. **Rolling time-split validation** - DONE\n",
    "8. **Remove fluctuating features, rank importance** - DONE\n",
    "9. **Credit bureau A (DANGEROUS tables)** - DONE\n",
    "10. **Ratio / interaction features** - DONE\n",
    "11. **other_1 bank account features** - DONE\n",
    "12. **credit_bureau_b_2 payment DPD** - DONE\n",
    "13. **tax_registry_b_1 + tax_registry_c_1** - DONE\n",
    "14. **person_1 (applicant + others) + person_2** - DONE\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.14.0' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# UPDATE THIS PATH to your local data directory\n",
    "DATA_DIR = Path(\"home_credit_data/csv_files/train\")\n",
    "\n",
    "TABLES = {\n",
    "    \"base\": DATA_DIR / \"train_base.csv\",\n",
    "    \"static_0_0\": DATA_DIR / \"train_static_0_0.csv\",\n",
    "    \"static_0_1\": DATA_DIR / \"train_static_0_1.csv\",\n",
    "    \"static_cb_0\": DATA_DIR / \"train_static_cb_0.csv\",\n",
    "    \"other_1\": DATA_DIR / \"train_other_1.csv\",\n",
    "    \"person_1\": DATA_DIR / \"train_person_1.csv\",\n",
    "    \"person_2\": DATA_DIR / \"train_person_2.csv\",\n",
    "    \"applprev_1_0\": DATA_DIR / \"train_applprev_1_0.csv\",\n",
    "    \"applprev_1_1\": DATA_DIR / \"train_applprev_1_1.csv\",\n",
    "    \"applprev_2\": DATA_DIR / \"train_applprev_2.csv\",\n",
    "    \"credit_bureau_b_1\": DATA_DIR / \"train_credit_bureau_b_1.csv\",\n",
    "    \"credit_bureau_b_2\": DATA_DIR / \"train_credit_bureau_b_2.csv\",\n",
    "    \"deposit_1\": DATA_DIR / \"train_deposit_1.csv\",\n",
    "    \"tax_registry_a_1\": DATA_DIR / \"train_tax_registry_a_1.csv\",\n",
    "    \"tax_registry_b_1\": DATA_DIR / \"train_tax_registry_b_1.csv\",\n",
    "    \"tax_registry_c_1\": DATA_DIR / \"train_tax_registry_c_1.csv\",\n",
    "    # credit_bureau_a shards (DANGEROUS — high depth, largest data source)\n",
    "    \"credit_bureau_a_1_0\": DATA_DIR / \"train_credit_bureau_a_1_0.csv\",\n",
    "    \"credit_bureau_a_1_1\": DATA_DIR / \"train_credit_bureau_a_1_1.csv\",\n",
    "    \"credit_bureau_a_1_2\": DATA_DIR / \"train_credit_bureau_a_1_2.csv\",\n",
    "    \"credit_bureau_a_1_3\": DATA_DIR / \"train_credit_bureau_a_1_3.csv\",\n",
    "    \"credit_bureau_a_2_0\": DATA_DIR / \"train_credit_bureau_a_2_0.csv\",\n",
    "    \"credit_bureau_a_2_1\": DATA_DIR / \"train_credit_bureau_a_2_1.csv\",\n",
    "    \"credit_bureau_a_2_2\": DATA_DIR / \"train_credit_bureau_a_2_2.csv\",\n",
    "    \"credit_bureau_a_2_3\": DATA_DIR / \"train_credit_bureau_a_2_3.csv\",\n",
    "    \"credit_bureau_a_2_4\": DATA_DIR / \"train_credit_bureau_a_2_4.csv\",\n",
    "    \"credit_bureau_a_2_5\": DATA_DIR / \"train_credit_bureau_a_2_5.csv\",\n",
    "    \"credit_bureau_a_2_6\": DATA_DIR / \"train_credit_bureau_a_2_6.csv\",\n",
    "    \"credit_bureau_a_2_7\": DATA_DIR / \"train_credit_bureau_a_2_7.csv\",\n",
    "    \"credit_bureau_a_2_8\": DATA_DIR / \"train_credit_bureau_a_2_8.csv\",\n",
    "    \"credit_bureau_a_2_9\": DATA_DIR / \"train_credit_bureau_a_2_9.csv\",\n",
    "    \"credit_bureau_a_2_10\": DATA_DIR / \"train_credit_bureau_a_2_10.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2) Load base table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pl.read_csv(TABLES[\"base\"])\n",
    "\n",
    "base_dates = (\n",
    "    base.select([\"case_id\", \"date_decision\"])\n",
    "    .with_columns(pl.col(\"date_decision\").str.strptime(pl.Date, strict=False))\n",
    ")\n",
    "\n",
    "print(f\"Base shape: {base.shape}\")\n",
    "print(f\"Target rate: {base['target'].mean():.4f}\")\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3) Feature building functions\n",
    "\n",
    "### Method 1: Leak-safe temporal filtering\n",
    "- Every record has an **event date** (e.g., contract start, last update)\n",
    "- We compute `age_years = (decision_date - event_date) / 365.25`\n",
    "- Only records with `age_years >= 0` are used — this ensures we never use future data that wouldn't be available at decision time\n",
    "- Without this, the model would cheat by seeing outcomes that haven't happened yet\n",
    "\n",
    "### Method 2: Active vs closed contract classification\n",
    "- **Active contracts:** maturity/end date is in the future relative to decision date, OR status column indicates active (e.g., status = \"A\")\n",
    "- **Closed contracts:** maturity/end date is in the past, OR status indicates closed (e.g., status in \"D\", \"K\", \"T\")\n",
    "- Separating these captures different risk signals — many active contracts = higher exposure, many closed = established history\n",
    "\n",
    "### Method 3: DPD (Days Past Due) conditional aggregation\n",
    "- DPD measures how late a borrower is on payments — the strongest credit risk signal\n",
    "- We create binary flags: `dpd >= 30` (early delinquency) and `dpd >= 90` (serious delinquency)\n",
    "- These are crossed with active/closed status to produce counts like \"number of active contracts with 30+ days overdue\"\n",
    "- DPD rate features (e.g., `dpd30_count / active_count`) capture delinquency proportion\n",
    "\n",
    "### Method 4: Time-windowed counts\n",
    "- Instead of a single total count, we bucket records by recency: **<=1 year**, **1-3 years**, **>3 years**\n",
    "- Recent events are more predictive than old ones — a DPD in the last year matters more than one 5 years ago\n",
    "- Custom windows (e.g., 0-0.5y, 0.5-2y, >2y) are tested per table to find the best split\n",
    "- Each window is crossed with active/closed for granularity\n",
    "\n",
    "### Method 5: Static table processing (1:1 join)\n",
    "- Static tables have exactly one row per applicant — no aggregation needed, just join\n",
    "- **Numeric columns** (suffixes L, P, A, T): cast to float (some load as strings due to mixed types across shards)\n",
    "- **Date columns** (suffix D): converted to **days since decision** — gives relative temporal features (e.g., \"last delinquency was 120 days ago\")\n",
    "- **Categorical columns** (suffix M): dropped — high cardinality, low signal for tree models without encoding\n",
    "- **Shard handling**: some static tables are split across files (e.g., static_0_0 + static_0_1) with schema mismatches — all columns are cast to String before concatenation, then properly typed\n",
    "\n",
    "### Method 6: Rolling time-split validation\n",
    "- Standard k-fold CV doesn't test temporal stability — the key metric for this competition\n",
    "- We use **3 time cuts** at WEEK_NUM = 50, 60, 70: train on weeks before the cut, validate on weeks after\n",
    "- This simulates deploying a model and testing if it holds up on future data\n",
    "- We report mean, std, min, max AUC across cuts — low std = stable model\n",
    "\n",
    "### Method 7: Incremental block evaluation\n",
    "- Features are added one block at a time, not all at once\n",
    "- After adding a block, we re-run the rolling evaluation\n",
    "- If AUC improves: keep the block. If it drops or stays flat: drop it\n",
    "- This prevents noisy/redundant features from diluting signal\n",
    "\n",
    "### Method 8: Credit bureau A processing (DANGEROUS tables)\n",
    "- **16M rows** (depth-1, contract level) + **188M rows** (depth-2, payment level) across 15 shards\n",
    "- Data comes from **two credit bureaus** filling alternate paired columns — coalesced to get best available value\n",
    "- 69 schema mismatches across shards — all columns cast to String before concat, then typed\n",
    "- **a_1 features (contract level):** active/closed contracts, DPD counts, time windows, PLUS numeric aggregations (max/mean of debt, overdue amounts, outstanding balances)\n",
    "- **a_2 features (payment level):** per-payment DPD max/mean/counts, overdue amounts — processed shard-by-shard to avoid OOM on 188M rows\n",
    "\n",
    "### Method 9: Ratio / interaction features\n",
    "- Raw amounts (income, debt, credit) vary wildly across applicants — ratios normalize them\n",
    "- **Debt-to-income:** current debt / income — measures leverage, higher = riskier\n",
    "- **Annuity-to-income:** monthly payment / income — measures affordability\n",
    "- **Credit utilization:** current debt / credit amount — measures how much of credit line is used\n",
    "- **DPD installment fraction:** installments with DPD / total installments — measures payment discipline\n",
    "- **DPD trend:** recent DPD / longer-term DPD — values >1 indicate worsening behavior\n",
    "- Safe division: returns null when denominator is zero or null, avoiding inf/NaN pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contract_features(\n",
    "    table_path: Path,\n",
    "    prefix: str,\n",
    "    base_dates: pl.DataFrame,\n",
    "    event_date_cols: list[str],\n",
    "    dpd_col: str | None = None,\n",
    "    active_flag_expr: pl.Expr | None = None,\n",
    "    closed_flag_expr: pl.Expr | None = None,\n",
    "):\n",
    "    df = pl.read_csv(table_path).join(base_dates, on=\"case_id\", how=\"left\")\n",
    "\n",
    "    parse_exprs = [pl.col(c).str.strptime(pl.Date, strict=False).alias(c) for c in event_date_cols if c in df.columns]\n",
    "    cast_exprs = [pl.col(dpd_col).cast(pl.Float64, strict=False).alias(dpd_col)] if (dpd_col is not None and dpd_col in df.columns) else []\n",
    "    df = df.with_columns(parse_exprs + cast_exprs)\n",
    "\n",
    "    available_dates = [pl.col(c) for c in event_date_cols if c in df.columns]\n",
    "    if len(available_dates) > 0:\n",
    "        df = df.with_columns(pl.coalesce(available_dates).alias(\"event_date\"))\n",
    "        df = df.with_columns(((pl.col(\"date_decision\") - pl.col(\"event_date\")).dt.total_days() / 365.25).alias(\"age_years\"))\n",
    "        known_mask = (pl.col(\"age_years\") >= 0)\n",
    "    else:\n",
    "        df = df.with_columns([\n",
    "            pl.lit(None).cast(pl.Date).alias(\"event_date\"),\n",
    "            pl.lit(None).cast(pl.Float64).alias(\"age_years\"),\n",
    "        ])\n",
    "        known_mask = pl.lit(True)\n",
    "\n",
    "    if active_flag_expr is None:\n",
    "        active_flag_expr = pl.lit(False)\n",
    "    if closed_flag_expr is None:\n",
    "        closed_flag_expr = pl.lit(False)\n",
    "\n",
    "    df = df.with_columns([\n",
    "        active_flag_expr.fill_null(False).alias(\"is_active\"),\n",
    "        closed_flag_expr.fill_null(False).alias(\"is_closed\"),\n",
    "    ])\n",
    "\n",
    "    if dpd_col is not None and dpd_col in df.columns:\n",
    "        df = df.with_columns([\n",
    "            (pl.col(dpd_col) >= 30).fill_null(False).alias(\"dpd30\"),\n",
    "            (pl.col(dpd_col) >= 90).fill_null(False).alias(\"dpd90\"),\n",
    "        ])\n",
    "    else:\n",
    "        df = df.with_columns([\n",
    "            pl.lit(False).alias(\"dpd30\"),\n",
    "            pl.lit(False).alias(\"dpd90\"),\n",
    "        ])\n",
    "\n",
    "    agg = df.group_by(\"case_id\").agg([\n",
    "        pl.len().alias(f\"{prefix}_row_count_all\"),\n",
    "        known_mask.sum().alias(f\"{prefix}_known_count\"),\n",
    "\n",
    "        (known_mask & pl.col(\"is_active\")).sum().alias(f\"{prefix}_active_count_all\"),\n",
    "        (known_mask & pl.col(\"is_closed\")).sum().alias(f\"{prefix}_closed_count_all\"),\n",
    "\n",
    "        (known_mask & pl.col(\"is_active\") & pl.col(\"dpd30\")).sum().alias(f\"{prefix}_active_dpd30_count_all\"),\n",
    "        (known_mask & pl.col(\"is_active\") & pl.col(\"dpd90\")).sum().alias(f\"{prefix}_active_dpd90_count_all\"),\n",
    "        (known_mask & pl.col(\"is_closed\") & pl.col(\"dpd30\")).sum().alias(f\"{prefix}_closed_dpd30_count_all\"),\n",
    "        (known_mask & pl.col(\"is_closed\") & pl.col(\"dpd90\")).sum().alias(f\"{prefix}_closed_dpd90_count_all\"),\n",
    "\n",
    "        (known_mask & pl.col(\"is_active\") & (pl.col(\"age_years\") <= 1)).sum().alias(f\"{prefix}_active_count_le1y\"),\n",
    "        (known_mask & pl.col(\"is_closed\") & (pl.col(\"age_years\") <= 1)).sum().alias(f\"{prefix}_closed_count_le1y\"),\n",
    "\n",
    "        (known_mask & pl.col(\"is_active\") & (pl.col(\"age_years\") > 1) & (pl.col(\"age_years\") <= 3)).sum().alias(f\"{prefix}_active_count_1to3y\"),\n",
    "        (known_mask & pl.col(\"is_closed\") & (pl.col(\"age_years\") > 1) & (pl.col(\"age_years\") <= 3)).sum().alias(f\"{prefix}_closed_count_1to3y\"),\n",
    "\n",
    "        (known_mask & pl.col(\"is_active\") & (pl.col(\"age_years\") > 3)).sum().alias(f\"{prefix}_active_count_gt3y\"),\n",
    "        (known_mask & pl.col(\"is_closed\") & (pl.col(\"age_years\") > 3)).sum().alias(f\"{prefix}_closed_count_gt3y\"),\n",
    "    ])\n",
    "\n",
    "    agg = agg.with_columns([\n",
    "        pl.when(pl.col(f\"{prefix}_active_count_all\") > 0)\n",
    "        .then(pl.col(f\"{prefix}_active_dpd30_count_all\") / pl.col(f\"{prefix}_active_count_all\"))\n",
    "        .otherwise(0.0)\n",
    "        .alias(f\"{prefix}_active_dpd30_rate\"),\n",
    "\n",
    "        pl.when(pl.col(f\"{prefix}_closed_count_all\") > 0)\n",
    "        .then(pl.col(f\"{prefix}_closed_dpd30_count_all\") / pl.col(f\"{prefix}_closed_count_all\"))\n",
    "        .otherwise(0.0)\n",
    "        .alias(f\"{prefix}_closed_dpd30_rate\"),\n",
    "    ])\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contract_features_custom_windows(\n",
    "    table_path: Path,\n",
    "    prefix: str,\n",
    "    base_dates: pl.DataFrame,\n",
    "    event_date_cols: list[str],\n",
    "    windows=((0,1),(1,3),(3,999)),\n",
    "    dpd_col: str | None = None,\n",
    "    active_flag_expr: pl.Expr | None = None,\n",
    "    closed_flag_expr: pl.Expr | None = None,\n",
    "):\n",
    "    df = pl.read_csv(table_path).join(base_dates, on=\"case_id\", how=\"left\")\n",
    "\n",
    "    parse_exprs = [pl.col(c).str.strptime(pl.Date, strict=False).alias(c) for c in event_date_cols if c in df.columns]\n",
    "    cast_exprs = [pl.col(dpd_col).cast(pl.Float64, strict=False).alias(dpd_col)] if (dpd_col is not None and dpd_col in df.columns) else []\n",
    "    df = df.with_columns(parse_exprs + cast_exprs)\n",
    "\n",
    "    dates = [pl.col(c) for c in event_date_cols if c in df.columns]\n",
    "    if len(dates) > 0:\n",
    "        df = df.with_columns([\n",
    "            pl.coalesce(dates).alias(\"event_date\"),\n",
    "            ((pl.col(\"date_decision\") - pl.coalesce(dates)).dt.total_days() / 365.25).alias(\"age_years\"),\n",
    "        ])\n",
    "        known = (pl.col(\"age_years\") >= 0)\n",
    "    else:\n",
    "        df = df.with_columns([pl.lit(None).cast(pl.Float64).alias(\"age_years\")])\n",
    "        known = pl.lit(True)\n",
    "\n",
    "    if active_flag_expr is None:\n",
    "        active_flag_expr = pl.lit(False)\n",
    "    if closed_flag_expr is None:\n",
    "        closed_flag_expr = pl.lit(False)\n",
    "\n",
    "    df = df.with_columns([\n",
    "        active_flag_expr.fill_null(False).alias(\"is_active\"),\n",
    "        closed_flag_expr.fill_null(False).alias(\"is_closed\"),\n",
    "    ])\n",
    "\n",
    "    aggs = [\n",
    "        pl.len().alias(f\"{prefix}_row_count_all\"),\n",
    "        (known & pl.col(\"is_active\")).sum().alias(f\"{prefix}_active_count_all\"),\n",
    "        (known & pl.col(\"is_closed\")).sum().alias(f\"{prefix}_closed_count_all\"),\n",
    "    ]\n",
    "\n",
    "    for lo, hi in windows:\n",
    "        tag = f\"{lo}to{hi}\" if hi < 999 else f\"gt{lo}\"\n",
    "        cond = (pl.col(\"age_years\") > lo) & (pl.col(\"age_years\") <= hi) if hi < 999 else (pl.col(\"age_years\") > lo)\n",
    "        aggs += [\n",
    "            (known & pl.col(\"is_active\") & cond).sum().alias(f\"{prefix}_active_count_{tag}\"),\n",
    "            (known & pl.col(\"is_closed\") & cond).sum().alias(f\"{prefix}_closed_count_{tag}\"),\n",
    "        ]\n",
    "\n",
    "    return df.group_by(\"case_id\").agg(aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_static_features(\n",
    "    table_paths: list[Path],\n",
    "    base_dates: pl.DataFrame,\n",
    "    prefix: str = \"s0\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Load and concat sharded static tables (1:1 with base).\n",
    "    - Numeric columns (L, P, A, T suffixes): keep as float\n",
    "    - Date columns (D suffix): convert to days-since-decision (relative time)\n",
    "    - Categorical columns (M suffix): drop (high cardinality, low signal)\n",
    "\n",
    "    Handles schema mismatches between shards by casting all non-case_id\n",
    "    columns to String before concat, then applying proper type conversions.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for p in table_paths:\n",
    "        raw = pl.read_csv(p)\n",
    "        # Cast all non-case_id columns to String to avoid schema conflicts between shards\n",
    "        cast_to_str = [pl.col(c).cast(pl.String) for c in raw.columns if c != \"case_id\"]\n",
    "        dfs.append(raw.with_columns(cast_to_str))\n",
    "    df = pl.concat(dfs)\n",
    "\n",
    "    # Join to get date_decision for relative date features\n",
    "    df = df.join(base_dates, on=\"case_id\", how=\"left\")\n",
    "\n",
    "    cols = [c for c in df.columns if c not in [\"case_id\", \"date_decision\"]]\n",
    "\n",
    "    # Classify columns by suffix\n",
    "    date_cols = [c for c in cols if c.endswith(\"D\")]\n",
    "    cat_cols = [c for c in cols if c.endswith(\"M\")]\n",
    "    num_cols = [c for c in cols if c not in date_cols and c not in cat_cols]\n",
    "\n",
    "    # Cast numeric columns (currently String) to float\n",
    "    cast_exprs = []\n",
    "    for c in num_cols:\n",
    "        cast_exprs.append(pl.col(c).cast(pl.Float64, strict=False).alias(f\"{prefix}_{c}\"))\n",
    "\n",
    "    # Convert date columns (currently String) to days since decision\n",
    "    date_exprs = []\n",
    "    for c in date_cols:\n",
    "        date_exprs.append(\n",
    "            (\n",
    "                (pl.col(\"date_decision\") - pl.col(c).str.strptime(pl.Date, strict=False))\n",
    "                .dt.total_days()\n",
    "                .cast(pl.Float64)\n",
    "            ).alias(f\"{prefix}_{c}_days\")\n",
    "        )\n",
    "\n",
    "    df = df.with_columns(cast_exprs + date_exprs)\n",
    "\n",
    "    keep_cols = (\n",
    "        [\"case_id\"]\n",
    "        + [f\"{prefix}_{c}\" for c in num_cols]\n",
    "        + [f\"{prefix}_{c}_days\" for c in date_cols]\n",
    "    )\n",
    "\n",
    "    return df.select(keep_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Method 8: Credit bureau A processing (DANGEROUS tables)\n",
    "- **16M rows** (depth-1, contract level) + **188M rows** (depth-2, payment level) across 15 shards\n",
    "- Data comes from **two credit bureaus** filling alternate paired columns — coalesced to get best available value\n",
    "- 69 schema mismatches across shards — all columns cast to String before concat, then typed\n",
    "- **a_1 features (contract level):** active/closed contracts, DPD counts, time windows, PLUS numeric aggregations (max/mean of debt, overdue amounts, outstanding balances)\n",
    "- **a_2 features (payment level):** per-payment DPD max/mean/counts, overdue amounts — processed shard-by-shard to avoid OOM on 188M rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cba1_features(shard_paths: list[Path], base_dates: pl.DataFrame, prefix: str = \"cba1\"):\n",
    "    \"\"\"\n",
    "    credit_bureau_a depth-1: contract-level records from external credit bureaus.\n",
    "    Two bureaus fill alternate paired columns — we coalesce to get best available value.\n",
    "    69 schema mismatches across shards — cast all to String before concat.\n",
    "\n",
    "    Produces:\n",
    "      - Count features: total contracts, active/closed, DPD>0/30/90 counts, time-windowed\n",
    "      - Numeric aggregations: max/mean of DPD, debt outstanding, overdue amounts\n",
    "      - Rate features: DPD30 rate for active and closed contracts\n",
    "    \"\"\"\n",
    "    # Load and concat shards (cast to String to handle schema mismatches)\n",
    "    dfs = []\n",
    "    for p in shard_paths:\n",
    "        raw = pl.read_csv(p)\n",
    "        cast_to_str = [pl.col(c).cast(pl.String) for c in raw.columns if c != \"case_id\"]\n",
    "        dfs.append(raw.with_columns(cast_to_str))\n",
    "    df = pl.concat(dfs)\n",
    "    print(f\"  {prefix} raw: {df.shape}\")\n",
    "\n",
    "    # Join base dates\n",
    "    df = df.join(base_dates, on=\"case_id\", how=\"left\")\n",
    "\n",
    "    # Coalesce paired columns (two credit bureaus fill alternates)\n",
    "    df = df.with_columns([\n",
    "        # DPD\n",
    "        pl.coalesce([\n",
    "            pl.col(\"dpdmax_139P\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"dpdmax_757P\").cast(pl.Float64, strict=False),\n",
    "        ]).alias(\"dpd_max\"),\n",
    "        # Credit end date (for active/closed)\n",
    "        pl.coalesce([\n",
    "            pl.col(\"dateofcredend_289D\").str.strptime(pl.Date, strict=False),\n",
    "            pl.col(\"dateofcredend_353D\").str.strptime(pl.Date, strict=False),\n",
    "        ]).alias(\"cred_end_date\"),\n",
    "        # Last update (for age/recency)\n",
    "        pl.coalesce([\n",
    "            pl.col(\"lastupdate_1112D\").str.strptime(pl.Date, strict=False),\n",
    "            pl.col(\"lastupdate_388D\").str.strptime(pl.Date, strict=False),\n",
    "        ]).alias(\"last_update\"),\n",
    "        # Amounts\n",
    "        pl.col(\"debtoutstand_525A\").cast(pl.Float64, strict=False).alias(\"debt_outstanding\"),\n",
    "        pl.col(\"debtoverdue_47A\").cast(pl.Float64, strict=False).alias(\"debt_overdue\"),\n",
    "        pl.coalesce([\n",
    "            pl.col(\"overdueamountmax_155A\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"overdueamountmax_35A\").cast(pl.Float64, strict=False),\n",
    "        ]).alias(\"overdue_max\"),\n",
    "        pl.coalesce([\n",
    "            pl.col(\"totaldebtoverduevalue_178A\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"totaldebtoverduevalue_718A\").cast(pl.Float64, strict=False),\n",
    "        ]).alias(\"total_debt_overdue\"),\n",
    "        pl.coalesce([\n",
    "            pl.col(\"outstandingamount_354A\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"outstandingamount_362A\").cast(pl.Float64, strict=False),\n",
    "        ]).alias(\"outstanding_amount\"),\n",
    "        pl.coalesce([\n",
    "            pl.col(\"numberofoverdueinstlmax_1039L\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"numberofoverdueinstlmax_1151L\").cast(pl.Float64, strict=False),\n",
    "        ]).alias(\"num_overdue_instl_max\"),\n",
    "        pl.coalesce([\n",
    "            pl.col(\"numberofoverdueinstls_725L\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"numberofoverdueinstls_834L\").cast(pl.Float64, strict=False),\n",
    "        ]).alias(\"num_overdue_instls\"),\n",
    "    ])\n",
    "\n",
    "    # Compute age in years from last_update\n",
    "    df = df.with_columns(\n",
    "        ((pl.col(\"date_decision\") - pl.col(\"last_update\")).dt.total_days() / 365.25).alias(\"age_years\")\n",
    "    )\n",
    "    known = pl.col(\"age_years\") >= 0\n",
    "\n",
    "    # Active/closed based on credit end date\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"cred_end_date\") >= pl.col(\"date_decision\")).fill_null(False).alias(\"is_active\"),\n",
    "        (pl.col(\"cred_end_date\") < pl.col(\"date_decision\")).fill_null(False).alias(\"is_closed\"),\n",
    "    ])\n",
    "\n",
    "    # DPD flags\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"dpd_max\") > 0).fill_null(False).alias(\"dpd_any\"),\n",
    "        (pl.col(\"dpd_max\") >= 30).fill_null(False).alias(\"dpd30\"),\n",
    "        (pl.col(\"dpd_max\") >= 90).fill_null(False).alias(\"dpd90\"),\n",
    "    ])\n",
    "\n",
    "    # Aggregate\n",
    "    agg = df.group_by(\"case_id\").agg([\n",
    "        # Count features\n",
    "        pl.len().alias(f\"{prefix}_contract_count\"),\n",
    "        known.sum().alias(f\"{prefix}_known_count\"),\n",
    "        (known & pl.col(\"is_active\")).sum().alias(f\"{prefix}_active_count\"),\n",
    "        (known & pl.col(\"is_closed\")).sum().alias(f\"{prefix}_closed_count\"),\n",
    "\n",
    "        # DPD count features\n",
    "        (known & pl.col(\"dpd_any\")).sum().alias(f\"{prefix}_dpd_any_count\"),\n",
    "        (known & pl.col(\"dpd30\")).sum().alias(f\"{prefix}_dpd30_count\"),\n",
    "        (known & pl.col(\"dpd90\")).sum().alias(f\"{prefix}_dpd90_count\"),\n",
    "        (known & pl.col(\"is_active\") & pl.col(\"dpd30\")).sum().alias(f\"{prefix}_active_dpd30_count\"),\n",
    "        (known & pl.col(\"is_active\") & pl.col(\"dpd90\")).sum().alias(f\"{prefix}_active_dpd90_count\"),\n",
    "        (known & pl.col(\"is_closed\") & pl.col(\"dpd30\")).sum().alias(f\"{prefix}_closed_dpd30_count\"),\n",
    "        (known & pl.col(\"is_closed\") & pl.col(\"dpd90\")).sum().alias(f\"{prefix}_closed_dpd90_count\"),\n",
    "\n",
    "        # Time-windowed counts\n",
    "        (known & pl.col(\"is_active\") & (pl.col(\"age_years\") <= 1)).sum().alias(f\"{prefix}_active_le1y\"),\n",
    "        (known & pl.col(\"is_closed\") & (pl.col(\"age_years\") <= 1)).sum().alias(f\"{prefix}_closed_le1y\"),\n",
    "        (known & pl.col(\"is_active\") & (pl.col(\"age_years\") > 1) & (pl.col(\"age_years\") <= 3)).sum().alias(f\"{prefix}_active_1to3y\"),\n",
    "        (known & pl.col(\"is_closed\") & (pl.col(\"age_years\") > 1) & (pl.col(\"age_years\") <= 3)).sum().alias(f\"{prefix}_closed_1to3y\"),\n",
    "        (known & pl.col(\"is_active\") & (pl.col(\"age_years\") > 3)).sum().alias(f\"{prefix}_active_gt3y\"),\n",
    "        (known & pl.col(\"is_closed\") & (pl.col(\"age_years\") > 3)).sum().alias(f\"{prefix}_closed_gt3y\"),\n",
    "\n",
    "        # Numeric aggregations (max/mean of key amounts)\n",
    "        pl.col(\"dpd_max\").max().alias(f\"{prefix}_dpd_max\"),\n",
    "        pl.col(\"dpd_max\").mean().alias(f\"{prefix}_dpd_mean\"),\n",
    "        pl.col(\"debt_outstanding\").max().alias(f\"{prefix}_debt_outstanding_max\"),\n",
    "        pl.col(\"debt_outstanding\").mean().alias(f\"{prefix}_debt_outstanding_mean\"),\n",
    "        pl.col(\"debt_overdue\").max().alias(f\"{prefix}_debt_overdue_max\"),\n",
    "        pl.col(\"debt_overdue\").mean().alias(f\"{prefix}_debt_overdue_mean\"),\n",
    "        pl.col(\"overdue_max\").max().alias(f\"{prefix}_overdue_max_max\"),\n",
    "        pl.col(\"overdue_max\").mean().alias(f\"{prefix}_overdue_max_mean\"),\n",
    "        pl.col(\"total_debt_overdue\").max().alias(f\"{prefix}_total_debt_overdue_max\"),\n",
    "        pl.col(\"total_debt_overdue\").sum().alias(f\"{prefix}_total_debt_overdue_sum\"),\n",
    "        pl.col(\"outstanding_amount\").max().alias(f\"{prefix}_outstanding_max\"),\n",
    "        pl.col(\"outstanding_amount\").sum().alias(f\"{prefix}_outstanding_sum\"),\n",
    "        pl.col(\"num_overdue_instl_max\").max().alias(f\"{prefix}_num_overdue_instl_max\"),\n",
    "        pl.col(\"num_overdue_instls\").max().alias(f\"{prefix}_num_overdue_instls_max\"),\n",
    "        pl.col(\"num_overdue_instls\").sum().alias(f\"{prefix}_num_overdue_instls_sum\"),\n",
    "    ])\n",
    "\n",
    "    # Rate features\n",
    "    agg = agg.with_columns([\n",
    "        pl.when(pl.col(f\"{prefix}_active_count\") > 0)\n",
    "            .then(pl.col(f\"{prefix}_active_dpd30_count\") / pl.col(f\"{prefix}_active_count\"))\n",
    "            .otherwise(0.0).alias(f\"{prefix}_active_dpd30_rate\"),\n",
    "        pl.when(pl.col(f\"{prefix}_closed_count\") > 0)\n",
    "            .then(pl.col(f\"{prefix}_closed_dpd30_count\") / pl.col(f\"{prefix}_closed_count\"))\n",
    "            .otherwise(0.0).alias(f\"{prefix}_closed_dpd30_rate\"),\n",
    "        pl.when(pl.col(f\"{prefix}_known_count\") > 0)\n",
    "            .then(pl.col(f\"{prefix}_dpd_any_count\") / pl.col(f\"{prefix}_known_count\"))\n",
    "            .otherwise(0.0).alias(f\"{prefix}_dpd_any_rate\"),\n",
    "    ])\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cba2_features(shard_paths: list[Path], prefix: str = \"cba2\"):\n",
    "    \"\"\"\n",
    "    credit_bureau_a depth-2: payment-level records. 188M rows across 11 shards.\n",
    "    Processed shard-by-shard to avoid OOM: aggregate each shard to case_id level,\n",
    "    then re-aggregate across shards.\n",
    "\n",
    "    Key columns: pmts_dpd_1073P (payment DPD), pmts_overdue_1140A (overdue amount).\n",
    "    \"\"\"\n",
    "    shard_aggs = []\n",
    "    for i, p in enumerate(shard_paths):\n",
    "        df = pl.scan_csv(p).select([\n",
    "            \"case_id\",\n",
    "            pl.col(\"pmts_dpd_1073P\").cast(pl.Float64, strict=False).alias(\"pmt_dpd\"),\n",
    "            pl.col(\"pmts_overdue_1140A\").cast(pl.Float64, strict=False).alias(\"pmt_overdue\"),\n",
    "        ])\n",
    "\n",
    "        shard_agg = df.group_by(\"case_id\").agg([\n",
    "            pl.len().alias(\"_n\"),\n",
    "            pl.col(\"pmt_dpd\").max().alias(\"_dpd_max\"),\n",
    "            pl.col(\"pmt_dpd\").sum().alias(\"_dpd_sum\"),\n",
    "            pl.col(\"pmt_dpd\").drop_nulls().len().alias(\"_dpd_nonnull\"),\n",
    "            (pl.col(\"pmt_dpd\") > 0).sum().alias(\"_dpd_gt0\"),\n",
    "            (pl.col(\"pmt_dpd\") >= 30).sum().alias(\"_dpd_ge30\"),\n",
    "            (pl.col(\"pmt_dpd\") >= 90).sum().alias(\"_dpd_ge90\"),\n",
    "            pl.col(\"pmt_overdue\").max().alias(\"_overdue_max\"),\n",
    "            pl.col(\"pmt_overdue\").sum().alias(\"_overdue_sum\"),\n",
    "            pl.col(\"pmt_overdue\").drop_nulls().len().alias(\"_overdue_nonnull\"),\n",
    "        ]).collect()\n",
    "\n",
    "        shard_aggs.append(shard_agg)\n",
    "        print(f\"  shard {i}: {shard_agg.height} case_ids aggregated\")\n",
    "\n",
    "    # Merge all shard aggregations\n",
    "    combined = pl.concat(shard_aggs)\n",
    "    print(f\"  Combined: {combined.shape}\")\n",
    "\n",
    "    # Re-aggregate across shards (same case_id may appear in multiple shards)\n",
    "    final = combined.group_by(\"case_id\").agg([\n",
    "        pl.col(\"_n\").sum().alias(f\"{prefix}_payment_count\"),\n",
    "        pl.col(\"_dpd_max\").max().alias(f\"{prefix}_dpd_max\"),\n",
    "        pl.col(\"_dpd_sum\").sum().alias(f\"{prefix}_dpd_sum\"),\n",
    "        pl.col(\"_dpd_nonnull\").sum().alias(f\"{prefix}_dpd_nonnull\"),\n",
    "        pl.col(\"_dpd_gt0\").sum().alias(f\"{prefix}_dpd_gt0_count\"),\n",
    "        pl.col(\"_dpd_ge30\").sum().alias(f\"{prefix}_dpd_ge30_count\"),\n",
    "        pl.col(\"_dpd_ge90\").sum().alias(f\"{prefix}_dpd_ge90_count\"),\n",
    "        pl.col(\"_overdue_max\").max().alias(f\"{prefix}_overdue_max\"),\n",
    "        pl.col(\"_overdue_sum\").sum().alias(f\"{prefix}_overdue_sum\"),\n",
    "        pl.col(\"_overdue_nonnull\").sum().alias(f\"{prefix}_overdue_nonnull\"),\n",
    "    ])\n",
    "\n",
    "    # Derived features\n",
    "    final = final.with_columns([\n",
    "        # Mean DPD across all payments\n",
    "        pl.when(pl.col(f\"{prefix}_dpd_nonnull\") > 0)\n",
    "            .then(pl.col(f\"{prefix}_dpd_sum\") / pl.col(f\"{prefix}_dpd_nonnull\"))\n",
    "            .otherwise(None).alias(f\"{prefix}_dpd_mean\"),\n",
    "        # Mean overdue amount\n",
    "        pl.when(pl.col(f\"{prefix}_overdue_nonnull\") > 0)\n",
    "            .then(pl.col(f\"{prefix}_overdue_sum\") / pl.col(f\"{prefix}_overdue_nonnull\"))\n",
    "            .otherwise(None).alias(f\"{prefix}_overdue_mean\"),\n",
    "        # DPD rate (fraction of payments with any DPD)\n",
    "        pl.when(pl.col(f\"{prefix}_dpd_nonnull\") > 0)\n",
    "            .then(pl.col(f\"{prefix}_dpd_gt0_count\") / pl.col(f\"{prefix}_dpd_nonnull\"))\n",
    "            .otherwise(0.0).alias(f\"{prefix}_dpd_rate\"),\n",
    "        # DPD30 rate\n",
    "        pl.when(pl.col(f\"{prefix}_dpd_nonnull\") > 0)\n",
    "            .then(pl.col(f\"{prefix}_dpd_ge30_count\") / pl.col(f\"{prefix}_dpd_nonnull\"))\n",
    "            .otherwise(0.0).alias(f\"{prefix}_dpd30_rate\"),\n",
    "    ])\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    final = final.drop([f\"{prefix}_dpd_sum\", f\"{prefix}_dpd_nonnull\", f\"{prefix}_overdue_nonnull\"])\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4) Build feature blocks\n",
    "\n",
    "### Block 1: cb1 (credit_bureau_b_1) — 16 features\n",
    "External credit bureau records. Active/closed by maturity date, DPD from `dpd_550P`.\n",
    "\n",
    "### Block 2: ap1w (applprev_1_1) — 9 features\n",
    "Previous applications with custom time windows (0-0.5y, 0.5-2y, 2+y).\n",
    "\n",
    "### Block 3: dep (deposit_1) — 16 features\n",
    "Deposit accounts. Active/closed by contract end date.\n",
    "\n",
    "### Block 4: taxa (tax_registry_a_1) — 16 features\n",
    "Tax records. All flagged active.\n",
    "\n",
    "### Block 5: ap2 (applprev_2) — 16 features\n",
    "Application counts only.\n",
    "\n",
    "### Block 6: s0 (static_0_0 + static_0_1) — ~159 features\n",
    "Applicant-level static data. 1:1 join. Numeric + date features.\n",
    "\n",
    "### Block 7: scb (static_cb_0) — ~47 features\n",
    "Credit bureau static data. 1:1 join.\n",
    "\n",
    "### Block 8: cba1 (credit_bureau_a depth-1) — ~36 features\n",
    "16M rows, 4 shards, paired columns coalesced. DPD counts, amounts, rates.\n",
    "\n",
    "### Block 9: cba2 (credit_bureau_a depth-2) — ~11 features\n",
    "188M rows, 11 shards. Per-payment DPD and overdue amounts.\n",
    "\n",
    "### Block 10: ratio (derived from s0) — 12 features\n",
    "Income ratios, credit utilization, payment discipline, DPD trends.\n",
    "\n",
    "### Block 11: oth (other_1) — 7 features\n",
    "Bank account activity. 51k rows, 1:1.\n",
    "\n",
    "### Block 12: cbb2 (credit_bureau_b_2) — 11 features\n",
    "1.3M payment-level records. DPD counts and overdue amounts.\n",
    "\n",
    "### Block 13: taxb (tax_registry_b_1) — 6 features\n",
    "1.1M rows. Deduction amounts (sum/max/mean), recency, presence flag.\n",
    "\n",
    "### Block 14: taxc (tax_registry_c_1) — 6 features\n",
    "3.3M rows. Payment amounts (sum/max/mean), recency, presence flag.\n",
    "\n",
    "### Block 15: p1 (person_1) — 12 features\n",
    "3M rows. **Applicant (num_group1=0):** income, age, children, employment, gender, housing.\n",
    "**Others:** count of associated persons.\n",
    "\n",
    "### Block 16: p2 (person_2) — 4 features\n",
    "1.6M rows, depth-2. Record count, distinct persons, unique roles, employment count.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Summary\n",
    "\n",
    "| Block | Source | Features | Key Signal |\n",
    "|-------|--------|----------|------------|\n",
    "| base | train_base | 1 | Temporal position (WEEK_NUM) |\n",
    "| cb1 | credit_bureau_b_1 | 16 | DPD delinquency |\n",
    "| ap1w | applprev_1_1 | 9 | Application velocity |\n",
    "| dep | deposit_1 | 16 | Financial stability |\n",
    "| taxa | tax_registry_a_1 | 16 | Income presence |\n",
    "| ap2 | applprev_2 | 16 | Application count |\n",
    "| s0 | static_0_0 + static_0_1 | ~159 | Income, debt, DPD history |\n",
    "| scb | static_cb_0 | ~47 | Credit queries, risk scores |\n",
    "| cba1 | credit_bureau_a_1_* | ~36 | External DPD, debt, overdue |\n",
    "| cba2 | credit_bureau_a_2_* | ~11 | Per-payment DPD |\n",
    "| ratio | derived from s0 | 12 | Financial health ratios |\n",
    "| oth | other_1 | 7 | Bank account activity |\n",
    "| cbb2 | credit_bureau_b_2 | 11 | Per-payment DPD (bureau B) |\n",
    "| taxb | tax_registry_b_1 | 6 | Tax deductions |\n",
    "| taxc | tax_registry_c_1 | 6 | Tax payments |\n",
    "| p1 | person_1 | 12 | Demographics, applicant vs others |\n",
    "| p2 | person_2 | 4 | Person record counts |\n",
    "| **Total** | | **~385** | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cb1: credit_bureau_b_1\n",
    "cb1_agg = build_contract_features(\n",
    "    table_path=TABLES[\"credit_bureau_b_1\"],\n",
    "    prefix=\"cb1\",\n",
    "    base_dates=base_dates,\n",
    "    event_date_cols=[\"lastupdate_260D\", \"contractdate_551D\"],\n",
    "    dpd_col=\"dpd_550P\",\n",
    "    active_flag_expr=(pl.col(\"contractmaturitydate_151D\").str.strptime(pl.Date, strict=False) >= pl.col(\"date_decision\")),\n",
    "    closed_flag_expr=(pl.col(\"contractmaturitydate_151D\").str.strptime(pl.Date, strict=False) < pl.col(\"date_decision\")),\n",
    ")\n",
    "print(f\"cb1: {cb1_agg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap1w: applprev_1_1 with best window scheme (<=0.5y, 0.5-2y, >2y)\n",
    "ap1w_agg = build_contract_features_custom_windows(\n",
    "    table_path=TABLES[\"applprev_1_1\"],\n",
    "    prefix=\"ap1w\",\n",
    "    base_dates=base_dates,\n",
    "    event_date_cols=[\"dateactivated_425D\", \"approvaldate_319D\", \"creationdate_885D\"],\n",
    "    windows=((0, 0.5), (0.5, 2), (2, 999)),\n",
    "    dpd_col=None,\n",
    "    active_flag_expr=pl.col(\"status_219L\").is_in([\"A\"]),\n",
    "    closed_flag_expr=pl.col(\"status_219L\").is_in([\"D\", \"K\", \"T\"]),\n",
    ")\n",
    "print(f\"ap1w: {ap1w_agg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep: deposit_1\n",
    "dep_agg = build_contract_features(\n",
    "    table_path=TABLES[\"deposit_1\"],\n",
    "    prefix=\"dep\",\n",
    "    base_dates=base_dates,\n",
    "    event_date_cols=[\"openingdate_313D\", \"contractenddate_991D\"],\n",
    "    dpd_col=None,\n",
    "    active_flag_expr=(pl.col(\"contractenddate_991D\") >= pl.col(\"date_decision\")),\n",
    "    closed_flag_expr=(pl.col(\"contractenddate_991D\") < pl.col(\"date_decision\")),\n",
    ")\n",
    "print(f\"dep: {dep_agg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxa: tax_registry_a_1\n",
    "taxa_agg = build_contract_features(\n",
    "    table_path=TABLES[\"tax_registry_a_1\"],\n",
    "    prefix=\"taxa\",\n",
    "    base_dates=base_dates,\n",
    "    event_date_cols=[\"recorddate_4527225D\"],\n",
    "    dpd_col=None,\n",
    "    active_flag_expr=pl.lit(True),\n",
    "    closed_flag_expr=pl.lit(False),\n",
    ")\n",
    "print(f\"taxa: {taxa_agg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap2: applprev_2 (no event dates or DPD, just row counts)\n",
    "ap2 = pl.read_csv(TABLES[\"applprev_2\"]).join(base_dates, on=\"case_id\", how=\"left\")\n",
    "\n",
    "ap2_agg = (\n",
    "    ap2.group_by(\"case_id\").agg([\n",
    "        pl.len().alias(\"ap2_row_count_all\"),\n",
    "        pl.len().alias(\"ap2_known_count\"),\n",
    "        pl.len().alias(\"ap2_active_count_all\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_closed_count_all\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_active_dpd30_count_all\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_active_dpd90_count_all\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_closed_dpd30_count_all\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_closed_dpd90_count_all\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_active_count_le1y\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_closed_count_le1y\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_active_count_1to3y\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_closed_count_1to3y\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_active_count_gt3y\"),\n",
    "        pl.lit(0).sum().cast(pl.Int64).alias(\"ap2_closed_count_gt3y\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.lit(0.0).alias(\"ap2_active_dpd30_rate\"),\n",
    "        pl.lit(0.0).alias(\"ap2_closed_dpd30_rate\"),\n",
    "    ])\n",
    ")\n",
    "print(f\"ap2: {ap2_agg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s0: static_0 (concat shards static_0_0 + static_0_1) — applicant-level features\n",
    "# 1:1 join, no aggregation. 167 cols: 91 numeric(L), 31 amounts(A), 22 DPD(P), 15 dates(D), 8 categorical(M dropped)\n",
    "s0_features = build_static_features(\n",
    "    table_paths=[TABLES[\"static_0_0\"], TABLES[\"static_0_1\"]],\n",
    "    base_dates=base_dates,\n",
    "    prefix=\"s0\",\n",
    ")\n",
    "print(f\"s0: {s0_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scb: static_cb_0 — credit bureau static features\n",
    "# 1:1 join, 52 cols: 32 numeric(L), 4 amounts(A), 2 scores(T), 9 dates(D), 5 categorical(M dropped)\n",
    "scb_features = build_static_features(\n",
    "    table_paths=[TABLES[\"static_cb_0\"]],\n",
    "    base_dates=base_dates,\n",
    "    prefix=\"scb\",\n",
    ")\n",
    "print(f\"scb: {scb_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cba1: credit_bureau_a depth-1 (contract level) — 16M rows, 4 shards, 79 cols\n",
    "# Coalesces paired columns from two credit bureaus, handles 69 schema mismatches\n",
    "# Features: contract counts, active/closed, DPD counts/rates, time windows, numeric aggregations\n",
    "print(\"Building cba1...\")\n",
    "cba1_paths = [TABLES[f\"credit_bureau_a_1_{i}\"] for i in range(4)]\n",
    "cba1_features = build_cba1_features(cba1_paths, base_dates, prefix=\"cba1\")\n",
    "print(f\"cba1: {cba1_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cba2: credit_bureau_a depth-2 (payment level) — 188M rows, 11 shards\n",
    "# Processed shard-by-shard to avoid OOM. Aggregates per-payment DPD and overdue amounts.\n",
    "# Features: payment count, DPD max/mean/rates, overdue max/mean/sum\n",
    "print(\"Building cba2...\")\n",
    "cba2_paths = [TABLES[f\"credit_bureau_a_2_{i}\"] for i in range(11)]\n",
    "cba2_features = build_cba2_features(cba2_paths, prefix=\"cba2\")\n",
    "print(f\"cba2: {cba2_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oth: other_1 — 1:1 table, 51k rows, 5 financial amount columns + 2 derived\n",
    "# Debit/deposit incoming/outgoing/balance — only ~3.3% of applicants have data\n",
    "oth_raw = pl.read_csv(TABLES[\"other_1\"]).select([\n",
    "    \"case_id\",\n",
    "    pl.col(\"amtdebitincoming_4809443A\").alias(\"oth_debit_in\"),\n",
    "    pl.col(\"amtdebitoutgoing_4809440A\").alias(\"oth_debit_out\"),\n",
    "    pl.col(\"amtdepositbalance_4809441A\").alias(\"oth_deposit_bal\"),\n",
    "    pl.col(\"amtdepositincoming_4809444A\").alias(\"oth_deposit_in\"),\n",
    "    pl.col(\"amtdepositoutgoing_4809442A\").alias(\"oth_deposit_out\"),\n",
    "]).with_columns([\n",
    "    # Net debit flow (incoming - outgoing): positive = more money coming in\n",
    "    (pl.col(\"oth_debit_in\") - pl.col(\"oth_debit_out\")).alias(\"oth_debit_net\"),\n",
    "    # Net deposit flow\n",
    "    (pl.col(\"oth_deposit_in\") - pl.col(\"oth_deposit_out\")).alias(\"oth_deposit_net\"),\n",
    "])\n",
    "print(f\"oth: {oth_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbb2: credit_bureau_b_2 — depth-2, 1.3M payment-level records\n",
    "# Columns: pmts_dpdvalue_108P (payment DPD), pmts_pmtsoverdue_635A (overdue amount)\n",
    "# Same pattern as cba2 but single file, no shard handling needed\n",
    "cbb2_raw = pl.read_csv(TABLES[\"credit_bureau_b_2\"]).select([\n",
    "    \"case_id\",\n",
    "    pl.col(\"pmts_dpdvalue_108P\").cast(pl.Float64, strict=False).alias(\"pmt_dpd\"),\n",
    "    pl.col(\"pmts_pmtsoverdue_635A\").cast(pl.Float64, strict=False).alias(\"pmt_overdue\"),\n",
    "])\n",
    "\n",
    "cbb2_features = cbb2_raw.group_by(\"case_id\").agg([\n",
    "    pl.len().alias(\"cbb2_payment_count\"),\n",
    "    pl.col(\"pmt_dpd\").max().alias(\"cbb2_dpd_max\"),\n",
    "    pl.col(\"pmt_dpd\").mean().alias(\"cbb2_dpd_mean\"),\n",
    "    (pl.col(\"pmt_dpd\") > 0).sum().alias(\"cbb2_dpd_gt0_count\"),\n",
    "    (pl.col(\"pmt_dpd\") >= 30).sum().alias(\"cbb2_dpd_ge30_count\"),\n",
    "    (pl.col(\"pmt_dpd\") >= 90).sum().alias(\"cbb2_dpd_ge90_count\"),\n",
    "    pl.col(\"pmt_overdue\").max().alias(\"cbb2_overdue_max\"),\n",
    "    pl.col(\"pmt_overdue\").mean().alias(\"cbb2_overdue_mean\"),\n",
    "    pl.col(\"pmt_overdue\").sum().alias(\"cbb2_overdue_sum\"),\n",
    "]).with_columns([\n",
    "    # DPD rate\n",
    "    pl.when(pl.col(\"cbb2_payment_count\") > 0)\n",
    "        .then(pl.col(\"cbb2_dpd_gt0_count\") / pl.col(\"cbb2_payment_count\"))\n",
    "        .otherwise(0.0).alias(\"cbb2_dpd_rate\"),\n",
    "    # DPD30 rate\n",
    "    pl.when(pl.col(\"cbb2_payment_count\") > 0)\n",
    "        .then(pl.col(\"cbb2_dpd_ge30_count\") / pl.col(\"cbb2_payment_count\"))\n",
    "        .otherwise(0.0).alias(\"cbb2_dpd30_rate\"),\n",
    "])\n",
    "\n",
    "print(f\"cbb2: {cbb2_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxb: tax_registry_b_1 — 1.1M rows, deduction amounts + dates\n",
    "taxb_raw = pl.read_csv(TABLES[\"tax_registry_b_1\"]).join(base_dates, on=\"case_id\", how=\"left\")\n",
    "taxb_raw = taxb_raw.with_columns([\n",
    "    pl.col(\"amount_4917619A\").cast(pl.Float64, strict=False).alias(\"amt\"),\n",
    "    ((pl.col(\"date_decision\") - pl.col(\"deductiondate_4917603D\").str.strptime(pl.Date, strict=False))\n",
    "     .dt.total_days().cast(pl.Float64)).alias(\"days_since\"),\n",
    "])\n",
    "taxb_features = taxb_raw.group_by(\"case_id\").agg([\n",
    "    pl.len().alias(\"taxb_count\"),\n",
    "    pl.col(\"amt\").sum().alias(\"taxb_amt_sum\"),\n",
    "    pl.col(\"amt\").max().alias(\"taxb_amt_max\"),\n",
    "    pl.col(\"amt\").mean().alias(\"taxb_amt_mean\"),\n",
    "    pl.col(\"days_since\").min().alias(\"taxb_most_recent_days\"),  # smallest = most recent\n",
    "]).with_columns([\n",
    "    # Flag: has any tax_b records\n",
    "    (pl.col(\"taxb_count\") > 0).cast(pl.Int8).alias(\"taxb_has_records\"),\n",
    "])\n",
    "print(f\"taxb: {taxb_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxc: tax_registry_c_1 — 3.3M rows, payment amounts + processing dates\n",
    "taxc_raw = pl.read_csv(TABLES[\"tax_registry_c_1\"]).join(base_dates, on=\"case_id\", how=\"left\")\n",
    "taxc_raw = taxc_raw.with_columns([\n",
    "    pl.col(\"pmtamount_36A\").cast(pl.Float64, strict=False).alias(\"pmt_amt\"),\n",
    "    ((pl.col(\"date_decision\") - pl.col(\"processingdate_168D\").str.strptime(pl.Date, strict=False))\n",
    "     .dt.total_days().cast(pl.Float64)).alias(\"days_since\"),\n",
    "])\n",
    "taxc_features = taxc_raw.group_by(\"case_id\").agg([\n",
    "    pl.len().alias(\"taxc_count\"),\n",
    "    pl.col(\"pmt_amt\").sum().alias(\"taxc_pmt_sum\"),\n",
    "    pl.col(\"pmt_amt\").max().alias(\"taxc_pmt_max\"),\n",
    "    pl.col(\"pmt_amt\").mean().alias(\"taxc_pmt_mean\"),\n",
    "    pl.col(\"days_since\").min().alias(\"taxc_most_recent_days\"),\n",
    "]).with_columns([\n",
    "    (pl.col(\"taxc_count\") > 0).cast(pl.Int8).alias(\"taxc_has_records\"),\n",
    "])\n",
    "print(f\"taxc: {taxc_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1: person_1 — 3M rows, 37 cols, depth-1\n",
    "# Split: applicant (num_group1=0) gets direct features, others get counted\n",
    "p1_raw = pl.read_csv(TABLES[\"person_1\"]).join(base_dates, on=\"case_id\", how=\"left\")\n",
    "\n",
    "# Applicant features (num_group1 == 0)\n",
    "p1_applicant = (\n",
    "    p1_raw.filter(pl.col(\"num_group1\") == 0)\n",
    "    .with_columns([\n",
    "        pl.col(\"mainoccupationinc_384A\").cast(pl.Float64, strict=False).alias(\"p1_applicant_income\"),\n",
    "        pl.col(\"childnum_185L\").cast(pl.Float64, strict=False).alias(\"p1_childnum\"),\n",
    "        pl.col(\"empl_employedtotal_800L\").cast(pl.Float64, strict=False).alias(\"p1_employed_total\"),\n",
    "        pl.col(\"gender_992L\").cast(pl.Float64, strict=False).alias(\"p1_gender\"),\n",
    "        pl.col(\"familystate_447L\").cast(pl.Float64, strict=False).alias(\"p1_family_state\"),\n",
    "        pl.col(\"housetype_905L\").cast(pl.Float64, strict=False).alias(\"p1_house_type\"),\n",
    "        pl.col(\"incometype_1044T\").cast(pl.Float64, strict=False).alias(\"p1_income_type\"),\n",
    "        pl.col(\"isreference_387L\").cast(pl.Float64, strict=False).alias(\"p1_is_reference\"),\n",
    "        # Age from birth date\n",
    "        ((pl.col(\"date_decision\") - pl.col(\"birth_259D\").str.strptime(pl.Date, strict=False))\n",
    "         .dt.total_days() / 365.25).alias(\"p1_age_years\"),\n",
    "        # Employment start (days since)\n",
    "        ((pl.col(\"date_decision\") - pl.col(\"empl_employedfrom_271D\").str.strptime(pl.Date, strict=False))\n",
    "         .dt.total_days().cast(pl.Float64)).alias(\"p1_employed_since_days\"),\n",
    "    ])\n",
    "    .select([\"case_id\", \"p1_applicant_income\", \"p1_childnum\", \"p1_employed_total\",\n",
    "             \"p1_gender\", \"p1_family_state\", \"p1_house_type\", \"p1_income_type\",\n",
    "             \"p1_is_reference\", \"p1_age_years\", \"p1_employed_since_days\"])\n",
    ")\n",
    "\n",
    "# Other persons count (num_group1 > 0)\n",
    "p1_others = (\n",
    "    p1_raw.filter(pl.col(\"num_group1\") > 0)\n",
    "    .group_by(\"case_id\").agg([\n",
    "        pl.len().alias(\"p1_other_person_count\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Merge applicant + others\n",
    "p1_features = p1_applicant.join(p1_others, on=\"case_id\", how=\"left\").with_columns(\n",
    "    pl.col(\"p1_other_person_count\").fill_null(0),\n",
    ")\n",
    "print(f\"p1: {p1_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2: person_2 — 1.6M rows, depth-2, mostly categorical\n",
    "# Low signal — just count records and unique roles per case_id\n",
    "p2_raw = pl.read_csv(TABLES[\"person_2\"])\n",
    "p2_features = p2_raw.group_by(\"case_id\").agg([\n",
    "    pl.len().alias(\"p2_record_count\"),\n",
    "    pl.col(\"num_group1\").n_unique().alias(\"p2_num_persons\"),  # distinct persons\n",
    "    pl.col(\"relatedpersons_role_762T\").cast(pl.String).n_unique().alias(\"p2_unique_roles\"),\n",
    "    pl.col(\"empls_employedfrom_796D\").drop_nulls().len().alias(\"p2_has_employment_count\"),\n",
    "])\n",
    "print(f\"p2: {p2_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 5) Merge all blocks into base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = (\n",
    "    base\n",
    "    .join(cb1_agg, on=\"case_id\", how=\"left\")\n",
    "    .join(ap1w_agg, on=\"case_id\", how=\"left\")\n",
    "    .join(dep_agg, on=\"case_id\", how=\"left\")\n",
    "    .join(taxa_agg, on=\"case_id\", how=\"left\")\n",
    "    .join(ap2_agg, on=\"case_id\", how=\"left\")\n",
    "    .join(s0_features, on=\"case_id\", how=\"left\")\n",
    "    .join(scb_features, on=\"case_id\", how=\"left\")\n",
    "    .join(cba1_features, on=\"case_id\", how=\"left\")\n",
    "    .join(cba2_features, on=\"case_id\", how=\"left\")\n",
    "    .join(oth_raw, on=\"case_id\", how=\"left\")\n",
    "    .join(cbb2_features, on=\"case_id\", how=\"left\")\n",
    "    .join(taxb_features, on=\"case_id\", how=\"left\")\n",
    "    .join(taxc_features, on=\"case_id\", how=\"left\")\n",
    "    .join(p1_features, on=\"case_id\", how=\"left\")\n",
    "    .join(p2_features, on=\"case_id\", how=\"left\")\n",
    "    .with_columns([\n",
    "        pl.col(\"^cb1_.*$\").fill_null(0),\n",
    "        pl.col(\"^ap1w_.*$\").fill_null(0),\n",
    "        pl.col(\"^dep_.*$\").fill_null(0),\n",
    "        pl.col(\"^taxa_.*$\").fill_null(0),\n",
    "        pl.col(\"^ap2_.*$\").fill_null(0),\n",
    "        pl.col(\"^cba1_.*$\").fill_null(0),\n",
    "        pl.col(\"^cba2_.*$\").fill_null(0),\n",
    "        pl.col(\"^cbb2_.*$\").fill_null(0),\n",
    "        pl.col(\"^p2_.*$\").fill_null(0),\n",
    "    ])\n",
    ")\n",
    "\n",
    "feature_cols = [c for c in model_df.columns if c not in [\"case_id\", \"date_decision\", \"MONTH\", \"target\"]]\n",
    "print(f\"Final shape: {model_df.shape}\")\n",
    "print(f\"Feature count: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10: Ratio / interaction features (computed from existing s0 columns)\n",
    "# No data loading — these are derived from static table features already joined\n",
    "\n",
    "def _safe_ratio(num_col, den_col, alias):\n",
    "    \"\"\"Division that returns null when denominator is zero/null.\"\"\"\n",
    "    return (\n",
    "        pl.when(pl.col(den_col).is_not_null() & (pl.col(den_col) > 0))\n",
    "        .then(pl.col(num_col) / pl.col(den_col))\n",
    "        .otherwise(None)\n",
    "        .alias(alias)\n",
    "    )\n",
    "\n",
    "model_df = model_df.with_columns([\n",
    "    # --- Income ratios (leverage & affordability) ---\n",
    "    _safe_ratio(\"s0_currdebt_22A\", \"s0_maininc_215A\", \"ratio_debt_to_income\"),\n",
    "    _safe_ratio(\"s0_annuity_780A\", \"s0_maininc_215A\", \"ratio_annuity_to_income\"),\n",
    "    _safe_ratio(\"s0_credamount_770A\", \"s0_maininc_215A\", \"ratio_credit_to_income\"),\n",
    "    _safe_ratio(\"s0_totaldebt_9A\", \"s0_maininc_215A\", \"ratio_total_debt_to_income\"),\n",
    "\n",
    "    # --- Credit utilization ---\n",
    "    _safe_ratio(\"s0_currdebt_22A\", \"s0_credamount_770A\", \"ratio_credit_utilization\"),\n",
    "    _safe_ratio(\"s0_sumoutstandtotal_3546847A\", \"s0_credamount_770A\", \"ratio_outstanding_to_credit\"),\n",
    "    _safe_ratio(\"s0_downpmt_116A\", \"s0_credamount_770A\", \"ratio_downpayment\"),\n",
    "\n",
    "    # --- Payment discipline ---\n",
    "    # Fraction of installments with DPD >= 10 days\n",
    "    pl.when(\n",
    "        (pl.col(\"s0_numinstlswithdpd10_728L\") + pl.col(\"s0_numinstlswithoutdpd_562L\")) > 0\n",
    "    ).then(\n",
    "        pl.col(\"s0_numinstlswithdpd10_728L\") /\n",
    "        (pl.col(\"s0_numinstlswithdpd10_728L\") + pl.col(\"s0_numinstlswithoutdpd_562L\"))\n",
    "    ).otherwise(None).alias(\"ratio_dpd_installment_frac\"),\n",
    "\n",
    "    # DPD trend: recent (3m) vs long-term (24m) — >1 means worsening\n",
    "    pl.when(pl.col(\"s0_maxdpdlast24m_143P\") > 0)\n",
    "    .then(pl.col(\"s0_maxdpdlast3m_392P\") / pl.col(\"s0_maxdpdlast24m_143P\"))\n",
    "    .otherwise(0.0).alias(\"ratio_dpd_trend_3m_vs_24m\"),\n",
    "\n",
    "    # --- Payment consistency ---\n",
    "    _safe_ratio(\"s0_avgpmtlast12m_4525200A\", \"s0_annuity_780A\", \"ratio_pmt_to_annuity\"),\n",
    "    _safe_ratio(\"s0_maxinstallast24m_3658928A\", \"s0_avginstallast24m_3658937A\", \"ratio_install_volatility\"),\n",
    "    _safe_ratio(\"s0_avgoutstandbalancel6m_4187114A\", \"s0_maxoutstandbalancel12m_4187113A\", \"ratio_outstand_6m_vs_12m\"),\n",
    "])\n",
    "\n",
    "ratio_cols = [c for c in model_df.columns if c.startswith(\"ratio_\")]\n",
    "print(f\"Ratio features added: {len(ratio_cols)}\")\n",
    "print(f\"  {ratio_cols}\")\n",
    "\n",
    "# Redefine feature_cols to include ratios\n",
    "feature_cols = [c for c in model_df.columns if c not in [\"case_id\", \"date_decision\", \"MONTH\", \"target\"]]\n",
    "print(f\"\\nTotal feature count: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 6) Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_auc_for_cut(df, feature_cols, cut_week):\n",
    "    tr = df.filter(pl.col(\"WEEK_NUM\") <= cut_week)\n",
    "    va = df.filter(pl.col(\"WEEK_NUM\") > cut_week)\n",
    "\n",
    "    Xtr = tr.select(feature_cols).to_pandas()\n",
    "    ytr = tr[\"target\"].to_pandas()\n",
    "    Xva = va.select(feature_cols).to_pandas()\n",
    "    yva = va[\"target\"].to_pandas()\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(max_depth=6, learning_rate=0.05, max_iter=200, random_state=42)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    p = clf.predict_proba(Xva)[:, 1]\n",
    "    return roc_auc_score(yva, p)\n",
    "\n",
    "\n",
    "def eval_stability(df, feature_cols, cuts=(50, 60, 70), label=\"model\"):\n",
    "    rows = []\n",
    "    for c in cuts:\n",
    "        auc = run_auc_for_cut(df, feature_cols, c)\n",
    "        rows.append({\"model\": label, \"cut_week\": c, \"auc\": auc})\n",
    "    out = pd.DataFrame(rows)\n",
    "    summary = {\n",
    "        \"model\": label,\n",
    "        \"mean_auc\": out[\"auc\"].mean(),\n",
    "        \"std_auc\": out[\"auc\"].std(ddof=0),\n",
    "        \"min_auc\": out[\"auc\"].min(),\n",
    "        \"max_auc\": out[\"auc\"].max(),\n",
    "    }\n",
    "    return out, pd.DataFrame([summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 7) Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail, summary = eval_stability(model_df, feature_cols, cuts=(50, 60, 70), label=\"all_features_with_static\")\n",
    "print(detail)\n",
    "print()\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
