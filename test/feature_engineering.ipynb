{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f667bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal base table rows: {base.shape[0]}\")\n",
    "print(f\"Original base table features: {base.shape[1]}\")\n",
    "print(f\"\\nEngineered dataset rows: {engineered_df_encoded.shape[0]}\")\n",
    "print(f\"Engineered dataset features: {engineered_df_encoded.shape[1]}\")\n",
    "print(f\"Features added: {engineered_df_encoded.shape[1] - base.shape[1]}\")\n",
    "print(f\"\\nFiltered dataset features: {engineered_df_filtered.shape[1]}\")\n",
    "print(f\"Features removed (due to high correlation): {engineered_df_encoded.shape[1] - engineered_df_filtered.shape[1]}\")\n",
    "\n",
    "print(f\"\\nFeature sources:\")\n",
    "print(f\"  - Base table: {base.shape[1]} features\")\n",
    "print(f\"  - Application history (applprev): {len(applprev_features.columns)} engineered features\")\n",
    "print(f\"  - Credit bureau: {len(creditbureau_features.columns)} engineered features\")\n",
    "print(f\"  - Person data: {len(person_features.columns)} features\")\n",
    "print(f\"  - Tax registry: {len(tax_features.columns)} features\")\n",
    "print(f\"  - Interaction features: {len(interaction_features.columns)} features\")\n",
    "\n",
    "print(f\"\\nOutput files created:\")\n",
    "print(f\"  1. test_engineered_features_full.csv - All engineered features with encoding\")\n",
    "print(f\"  2. test_engineered_features_filtered.csv - Features after correlation filtering\")\n",
    "print(f\"  3. test_engineered_features_scaled.csv - Scaled features for ML models\")\n",
    "print(f\"  4. test_engineered_features_metadata.csv - Feature metadata and missing value info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export different versions of the engineered features\n",
    "\n",
    "# 1. Full engineered dataset with encoded categorical variables\n",
    "output_file_1 = f'{test_folder}/test_engineered_features_full.csv'\n",
    "engineered_df_encoded.to_csv(output_file_1, index=False)\n",
    "print(f\"✓ Exported full engineered features to: {output_file_1}\")\n",
    "\n",
    "# 2. Filtered dataset (removed highly correlated features)\n",
    "output_file_2 = f'{test_folder}/test_engineered_features_filtered.csv'\n",
    "engineered_df_filtered.to_csv(output_file_2, index=False)\n",
    "print(f\"✓ Exported filtered engineered features to: {output_file_2}\")\n",
    "\n",
    "# 3. Scaled dataset (for models that benefit from scaling)\n",
    "output_file_3 = f'{test_folder}/test_engineered_features_scaled.csv'\n",
    "engineered_df_scaled.to_csv(output_file_3, index=False)\n",
    "print(f\"✓ Exported scaled engineered features to: {output_file_3}\")\n",
    "\n",
    "# 4. Feature metadata\n",
    "feature_metadata = pd.DataFrame({\n",
    "    'Feature': engineered_df_encoded.columns,\n",
    "    'Data_Type': engineered_df_encoded.dtypes.values,\n",
    "    'Missing_Count': engineered_df_encoded.isnull().sum().values,\n",
    "    'Missing_Percentage': (engineered_df_encoded.isnull().sum() / len(engineered_df_encoded) * 100).values\n",
    "})\n",
    "\n",
    "output_file_4 = f'{test_folder}/test_engineered_features_metadata.csv'\n",
    "feature_metadata.to_csv(output_file_4, index=False)\n",
    "print(f\"✓ Exported feature metadata to: {output_file_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111987a0",
   "metadata": {},
   "source": [
    "## 13. Export Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features to reduce multicollinearity\n",
    "def remove_highly_correlated_features(df, corr_matrix, threshold=0.95):\n",
    "    \"\"\"Remove one of each highly correlated pair\"\"\"\n",
    "    to_drop = set()\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                # Drop the second feature (j) to keep first ones\n",
    "                to_drop.add(corr_matrix.columns[j])\n",
    "    \n",
    "    print(f\"Removing {len(to_drop)} highly correlated features\")\n",
    "    df_filtered = df.drop(columns=list(to_drop), errors='ignore')\n",
    "    return df_filtered\n",
    "\n",
    "engineered_df_filtered = remove_highly_correlated_features(\n",
    "    engineered_df_encoded, correlation_matrix, threshold=0.95\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape after removing highly correlated features: {engineered_df_filtered.shape}\")\n",
    "print(f\"Original features: {engineered_df_encoded.shape[1]}, Filtered features: {engineered_df_filtered.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea89c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "numeric_features_df = engineered_df_encoded.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numeric_features_df.corr()\n",
    "\n",
    "# Find highly correlated features\n",
    "def find_highly_correlated_pairs(corr_matrix, threshold=0.95):\n",
    "    \"\"\"Find pairs of features with high correlation\"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                pairs.append({\n",
    "                    'Feature 1': corr_matrix.columns[i],\n",
    "                    'Feature 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    return pairs\n",
    "\n",
    "highly_correlated = find_highly_correlated_pairs(correlation_matrix, threshold=0.95)\n",
    "print(f\"Found {len(highly_correlated)} highly correlated pairs (>0.95 correlation):\")\n",
    "if highly_correlated:\n",
    "    for pair in highly_correlated[:10]:  # Show first 10\n",
    "        print(f\"  {pair['Feature 1']} <-> {pair['Feature 2']}: {pair['Correlation']:.4f}\")\n",
    "else:\n",
    "    print(\"  No highly correlated pairs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a4283",
   "metadata": {},
   "source": [
    "## 12. Feature Selection - Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling - apply StandardScaler to numeric features\n",
    "numeric_cols_all = engineered_df_encoded.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create a separate scaled version for features that might need scaling\n",
    "scaler = StandardScaler()\n",
    "engineered_df_scaled = engineered_df_encoded.copy()\n",
    "\n",
    "# Scale numeric features\n",
    "engineered_df_scaled[numeric_cols_all] = scaler.fit_transform(engineered_df_encoded[numeric_cols_all])\n",
    "\n",
    "print(f\"Features scaled using StandardScaler\")\n",
    "print(f\"Scaled dataset shape: {engineered_df_scaled.shape}\")\n",
    "print(f\"\\nScaled features statistics (first 5 columns):\")\n",
    "print(engineered_df_scaled.iloc[:, :5].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436edbc7",
   "metadata": {},
   "source": [
    "## 11. Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = engineered_df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns to encode: {categorical_cols}\")\n",
    "\n",
    "# Use label encoding for categorical features\n",
    "le_dict = {}\n",
    "engineered_df_encoded = engineered_df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Handle missing values before encoding\n",
    "    engineered_df_encoded[col] = engineered_df_encoded[col].fillna('Missing')\n",
    "    engineered_df_encoded[col] = le.fit_transform(engineered_df_encoded[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "print(f\"\\nFinal engineered dataset shape: {engineered_df_encoded.shape}\")\n",
    "print(f\"Data types after encoding:\")\n",
    "print(engineered_df_encoded.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092da56",
   "metadata": {},
   "source": [
    "## 10. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0306c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers_iqr(df, columns=None, multiplier=1.5):\n",
    "    \"\"\"Handle outliers using IQR method\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        # Cap outliers instead of removing\n",
    "        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply outlier handling to numeric features\n",
    "numeric_cols = engineered_df.select_dtypes(include=[np.number]).columns\n",
    "engineered_df[numeric_cols] = handle_outliers_iqr(engineered_df, numeric_cols, multiplier=1.5)\n",
    "\n",
    "print(\"Outliers handled using IQR method (capped rather than removed)\")\n",
    "print(f\"Dataset shape after outlier handling: {engineered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819ffda",
   "metadata": {},
   "source": [
    "## 9. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeebead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values in engineered dataset\n",
    "analyze_missing_values(engineered_df, \"Engineered Dataset\")\n",
    "\n",
    "# Fill missing values\n",
    "numeric_features = engineered_df.select_dtypes(include=[np.number]).columns\n",
    "categorical_features = engineered_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# For numeric features, use median imputation\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "engineered_df[numeric_features] = numeric_imputer.fit_transform(engineered_df[numeric_features])\n",
    "\n",
    "# For categorical features, use most_frequent imputation\n",
    "if len(categorical_features) > 0:\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    engineered_df[categorical_features] = categorical_imputer.fit_transform(engineered_df[categorical_features])\n",
    "\n",
    "print(f\"After imputation - Missing values: {engineered_df.isnull().sum().sum()}\")\n",
    "print(f\"Engineered dataset shape after imputation: {engineered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187cf71",
   "metadata": {},
   "source": [
    "## 8. Handle Missing Values in Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with base table\n",
    "engineered_df = base.copy()\n",
    "\n",
    "# Merge applprev features\n",
    "if len(applprev_features) > 0:\n",
    "    applprev_features_reindex = applprev_features.reindex(engineered_df['case_id'], fill_value=0)\n",
    "    engineered_df = pd.concat([engineered_df, applprev_features_reindex], axis=1)\n",
    "    print(f\"After applprev merge: {engineered_df.shape}\")\n",
    "\n",
    "# Merge credit bureau features\n",
    "if len(creditbureau_features) > 0:\n",
    "    cb_features_reindex = creditbureau_features.reindex(engineered_df['case_id'], fill_value=0)\n",
    "    engineered_df = pd.concat([engineered_df, cb_features_reindex], axis=1)\n",
    "    print(f\"After creditbureau merge: {engineered_df.shape}\")\n",
    "\n",
    "# Merge person features\n",
    "if len(person_features) > 0:\n",
    "    engineered_df = engineered_df.join(person_features, on='case_id', how='left')\n",
    "    print(f\"After person merge: {engineered_df.shape}\")\n",
    "\n",
    "# Merge tax features\n",
    "if len(tax_features) > 0:\n",
    "    engineered_df = engineered_df.join(tax_features, on='case_id', how='left')\n",
    "    print(f\"After tax merge: {engineered_df.shape}\")\n",
    "\n",
    "# Merge interaction features\n",
    "if len(interaction_features) > 0:\n",
    "    engineered_df = engineered_df.join(interaction_features, on='case_id', how='left')\n",
    "    print(f\"After interaction features merge: {engineered_df.shape}\")\n",
    "\n",
    "print(f\"\\nFinal engineered dataset shape: {engineered_df.shape}\")\n",
    "print(f\"Columns: {engineered_df.shape[1]}\")\n",
    "print(f\"Sample of engineered dataset:\")\n",
    "print(engineered_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10830d5",
   "metadata": {},
   "source": [
    "## 7. Merge All Features with Base Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f167d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "interaction_features = pd.DataFrame(index=base['case_id'])\n",
    "\n",
    "# Interaction between applprev and creditbureau if they share keys\n",
    "if len(applprev_features) > 0 and len(creditbureau_features) > 0:\n",
    "    # Reindex to align with base table\n",
    "    applprev_aligned = applprev_features.reindex(base['case_id'], fill_value=0)\n",
    "    creditbureau_aligned = creditbureau_features.reindex(base['case_id'], fill_value=0)\n",
    "    \n",
    "    # Create some key interactions\n",
    "    if 'applprev_count' in applprev_aligned.columns and 'cb_num_records' in creditbureau_aligned.columns:\n",
    "        interaction_features['applprev_cb_count_ratio'] = (\n",
    "            applprev_aligned['applprev_count'] / (creditbureau_aligned['cb_num_records'] + 1)\n",
    "        )\n",
    "    \n",
    "    if 'applprev_total_credit_amount' in applprev_aligned.columns and 'cb_total_outstanding' in creditbureau_aligned.columns:\n",
    "        interaction_features['credit_to_outstanding_ratio'] = (\n",
    "            applprev_aligned['applprev_total_credit_amount'] / (creditbureau_aligned['cb_total_outstanding'] + 1)\n",
    "        )\n",
    "\n",
    "print(f\"Interaction features shape: {interaction_features.shape}\")\n",
    "print(f\"Interaction features:\")\n",
    "print(interaction_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd0f6e",
   "metadata": {},
   "source": [
    "## 6. Create Interaction Features\n",
    "\n",
    "Generate interaction features to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d33be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tax registry table\n",
    "tax_features = pd.DataFrame(index=base['case_id'])\n",
    "\n",
    "if tax_registry is not None:\n",
    "    # Aggregate tax registry data\n",
    "    tax_unique = tax_registry.drop_duplicates(subset=['case_id'], keep='first')\n",
    "    \n",
    "    # Numeric features from tax registry\n",
    "    numeric_tax = tax_unique.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in numeric_tax:\n",
    "        if col != 'case_id':\n",
    "            tax_features[f'tax_{col}'] = tax_unique.set_index('case_id')[col]\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_tax = tax_unique.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in categorical_tax[:3]:\n",
    "        if col != 'case_id':\n",
    "            tax_features[f'tax_{col}'] = tax_unique.set_index('case_id')[col]\n",
    "    \n",
    "    print(f\"Tax registry features shape: {tax_features.shape}\")\n",
    "    print(f\"Tax registry features sample:\")\n",
    "    print(tax_features.head())\n",
    "else:\n",
    "    print(\"No tax registry table available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process person table\n",
    "person_features = pd.DataFrame(index=base['case_id'])\n",
    "\n",
    "if person is not None:\n",
    "    # Keep unique person records (in case of duplicates)\n",
    "    person_unique = person.drop_duplicates(subset=['case_id'], keep='first')\n",
    "    \n",
    "    # Numeric features from person table\n",
    "    numeric_person = person_unique.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in numeric_person:\n",
    "        if col != 'case_id':\n",
    "            person_features[f'person_{col}'] = person_unique.set_index('case_id')[col]\n",
    "    \n",
    "    # Categorical features from person table\n",
    "    categorical_person = person_unique.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in categorical_person[:5]:  # Limit to first 5 categorical columns to avoid explosion\n",
    "        if col != 'case_id':\n",
    "            person_features[f'person_{col}'] = person_unique.set_index('case_id')[col]\n",
    "    \n",
    "    print(f\"Person features shape: {person_features.shape}\")\n",
    "    print(f\"Person features sample:\")\n",
    "    print(person_features.head())\n",
    "else:\n",
    "    print(\"No person table available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295f58f",
   "metadata": {},
   "source": [
    "## 5. Create Features from Person and Tax Registry Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional credit bureau features - counts and key metrics\n",
    "creditbureau_features = pd.DataFrame(index=creditbureau['case_id'].unique())\n",
    "\n",
    "# Count number of credit bureau records\n",
    "creditbureau_features['cb_num_records'] = creditbureau.groupby('case_id').size()\n",
    "\n",
    "# Count contracts by status\n",
    "if 'contractst_545M' in creditbureau.columns:\n",
    "    contract_counts = creditbureau.groupby('case_id')['contractst_545M'].apply(\n",
    "        lambda x: pd.Series({\n",
    "            f'cb_contractst_{status}_count': (x == status).sum() \n",
    "            for status in x.dropna().unique()\n",
    "        })\n",
    "    ).fillna(0)\n",
    "    creditbureau_features = creditbureau_features.join(contract_counts)\n",
    "\n",
    "# Overdue amount features\n",
    "if 'overdueamount_31A' in creditbureau.columns:\n",
    "    creditbureau_features['cb_total_overdue_amount'] = creditbureau.groupby('case_id')['overdueamount_31A'].sum()\n",
    "    creditbureau_features['cb_max_overdue_amount'] = creditbureau.groupby('case_id')['overdueamount_31A'].max()\n",
    "    creditbureau_features['cb_avg_overdue_amount'] = creditbureau.groupby('case_id')['overdueamount_31A'].mean()\n",
    "\n",
    "# Outstanding debt features\n",
    "if 'outstandingamount_354A' in creditbureau.columns:\n",
    "    creditbureau_features['cb_total_outstanding'] = creditbureau.groupby('case_id')['outstandingamount_354A'].sum()\n",
    "    creditbureau_features['cb_max_outstanding'] = creditbureau.groupby('case_id')['outstandingamount_354A'].max()\n",
    "\n",
    "# DPD (Days Past Due) features\n",
    "if 'dpdmax_139P' in creditbureau.columns:\n",
    "    creditbureau_features['cb_max_dpd'] = creditbureau.groupby('case_id')['dpdmax_139P'].max()\n",
    "    creditbureau_features['cb_avg_dpd'] = creditbureau.groupby('case_id')['dpdmax_139P'].mean()\n",
    "\n",
    "print(f\"Credit bureau engineered features shape: {creditbureau_features.shape}\")\n",
    "print(f\"Sample engineered features:\")\n",
    "print(creditbureau_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns from credit bureau for aggregation\n",
    "numeric_cols_cb = creditbureau.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns in credit bureau (count: {len(numeric_cols_cb)}):\")\n",
    "print(numeric_cols_cb[:20])\n",
    "\n",
    "# Aggregate features by case_id\n",
    "creditbureau_agg = creditbureau.groupby('case_id')[numeric_cols_cb].agg([\n",
    "    'sum', 'mean', 'max', 'min', 'count'\n",
    "]).fillna(0)\n",
    "\n",
    "# Flatten column names\n",
    "creditbureau_agg.columns = ['cb_' + '_'.join(col).strip() for col in creditbureau_agg.columns]\n",
    "\n",
    "print(f\"\\nAggregated credit bureau features shape: {creditbureau_agg.shape}\")\n",
    "print(f\"Sample aggregated features:\")\n",
    "print(creditbureau_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd69f1",
   "metadata": {},
   "source": [
    "## 4. Create Numerical Features from Credit Bureau Table\n",
    "\n",
    "Engineer aggregated features from the credit bureau data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c87332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional application/previous features - counts and ratios\n",
    "applprev_features = applprev.groupby('case_id').agg({\n",
    "    'status_219L': 'count',  # Number of previous applications\n",
    "}).rename(columns={'status_219L': 'applprev_count'})\n",
    "\n",
    "# Count by status if available\n",
    "if 'status_219L' in applprev.columns:\n",
    "    status_counts = applprev.groupby('case_id')['status_219L'].apply(\n",
    "        lambda x: pd.Series({\n",
    "            f'applprev_status_{status}_count': (x == status).sum() \n",
    "            for status in x.unique() if status is not None\n",
    "        })\n",
    "    ).fillna(0)\n",
    "    applprev_features = applprev_features.join(status_counts)\n",
    "\n",
    "# Add amount-based features if available\n",
    "if 'credamount_590A' in applprev.columns:\n",
    "    applprev_features['applprev_total_credit_amount'] = applprev.groupby('case_id')['credamount_590A'].sum()\n",
    "    applprev_features['applprev_avg_credit_amount'] = applprev.groupby('case_id')['credamount_590A'].mean()\n",
    "    applprev_features['applprev_max_credit_amount'] = applprev.groupby('case_id')['credamount_590A'].max()\n",
    "\n",
    "if 'annuity_853A' in applprev.columns:\n",
    "    applprev_features['applprev_total_annuity'] = applprev.groupby('case_id')['annuity_853A'].sum()\n",
    "    applprev_features['applprev_avg_annuity'] = applprev.groupby('case_id')['annuity_853A'].mean()\n",
    "\n",
    "print(f\"Applprev engineered features shape: {applprev_features.shape}\")\n",
    "print(f\"Sample engineered features:\")\n",
    "print(applprev_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns from applprev for aggregation\n",
    "numeric_cols_applprev = applprev.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns in applprev (count: {len(numeric_cols_applprev)}):\")\n",
    "print(numeric_cols_applprev[:20])  # Show first 20\n",
    "\n",
    "# Aggregate features by case_id\n",
    "applprev_agg = applprev.groupby('case_id')[numeric_cols_applprev].agg([\n",
    "    'sum', 'mean', 'max', 'min', 'std', 'count'\n",
    "]).fillna(0)\n",
    "\n",
    "# Flatten column names\n",
    "applprev_agg.columns = ['_'.join(col).strip() for col in applprev_agg.columns]\n",
    "\n",
    "print(f\"\\nAggregated applprev features shape: {applprev_agg.shape}\")\n",
    "print(f\"Sample aggregated features:\")\n",
    "print(applprev_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5741e1b",
   "metadata": {},
   "source": [
    "## 3. Create Numerical Features from Application/Previous Table\n",
    "\n",
    "Engineer aggregated numerical features from the applprev table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df, name=\"\"):\n",
    "    \"\"\"Analyze missing values in a dataframe\"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing_Count': missing.values,\n",
    "        'Missing_Percentage': missing_pct.values\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(f\"\\n{name} - Missing Values Summary:\")\n",
    "        print(missing_df.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\\n{name} - No missing values found!\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Analyze missing values in each table\n",
    "analyze_missing_values(base, \"Base Table\")\n",
    "analyze_missing_values(applprev, \"Application/Previous Table\")\n",
    "analyze_missing_values(creditbureau, \"Credit Bureau Table\")\n",
    "if person is not None:\n",
    "    analyze_missing_values(person, \"Person Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea5b25",
   "metadata": {},
   "source": [
    "## 2. Handle Missing Values\n",
    "\n",
    "Analyze and handle missing values in the dataset using appropriate techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce02e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other important tables\n",
    "person_tables = []\n",
    "for file in sorted([f for f in os.listdir(test_folder) if f.startswith('test_person')]):\n",
    "    df = pd.read_csv(f'{test_folder}/{file}')\n",
    "    person_tables.append(df)\n",
    "    print(f\"Loaded {file}: shape {df.shape}\")\n",
    "\n",
    "person = pd.concat(person_tables, ignore_index=False) if person_tables else None\n",
    "print(f\"Combined person shape: {person.shape if person is not None else 'None'}\")\n",
    "\n",
    "# Load tax registry tables\n",
    "tax_registry = []\n",
    "for file in sorted([f for f in os.listdir(test_folder) if f.startswith('test_tax_registry')]):\n",
    "    df = pd.read_csv(f'{test_folder}/{file}')\n",
    "    tax_registry.append(df)\n",
    "    print(f\"Loaded {file}: shape {df.shape}\")\n",
    "\n",
    "tax_registry = pd.concat(tax_registry, ignore_index=False) if tax_registry else None\n",
    "print(f\"Combined tax registry shape: {tax_registry.shape if tax_registry is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credit bureau tables\n",
    "creditbureau_tables = []\n",
    "creditbureau_files = sorted([f for f in os.listdir(test_folder) if f.startswith('test_credit_bureau')])\n",
    "print(f\"Found {len(creditbureau_files)} credit bureau files\")\n",
    "\n",
    "for file in creditbureau_files:\n",
    "    df = pd.read_csv(f'{test_folder}/{file}')\n",
    "    creditbureau_tables.append(df)\n",
    "    print(f\"  {file}: shape {df.shape}\")\n",
    "\n",
    "# Combine all credit bureau tables\n",
    "creditbureau = pd.concat(creditbureau_tables, ignore_index=False)\n",
    "print(f\"\\nCombined credit bureau shape: {creditbureau.shape}\")\n",
    "print(f\"Credit bureau columns (first 10): {creditbureau.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fd24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load application/previous credit tables\n",
    "applprev_tables = []\n",
    "applprev_files = sorted([f for f in os.listdir(test_folder) if f.startswith('test_applprev')])\n",
    "print(f\"\\nFound {len(applprev_files)} applprev files: {applprev_files}\")\n",
    "\n",
    "for file in applprev_files:\n",
    "    df = pd.read_csv(f'{test_folder}/{file}')\n",
    "    applprev_tables.append(df)\n",
    "    print(f\"  {file}: shape {df.shape}\")\n",
    "\n",
    "# Combine all applprev tables\n",
    "applprev = pd.concat(applprev_tables, ignore_index=False)\n",
    "print(f\"\\nCombined applprev shape: {applprev.shape}\")\n",
    "print(f\"Applprev columns (first 10): {applprev.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ae4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base table\n",
    "base = pd.read_csv(f'{test_folder}/test_base.csv')\n",
    "print(\"Base table shape:\", base.shape)\n",
    "print(\"\\nBase table columns:\")\n",
    "print(base.columns.tolist())\n",
    "print(\"\\nBase table sample:\")\n",
    "print(base.head())\n",
    "print(\"\\nBase table info:\")\n",
    "print(base.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49084a",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import glob\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Define the test folder path\n",
    "test_folder = '/Users/ryanzhang/Documents/Uni stuff/sc4000/home-credit-credit-risk-model-stability/csv_files/test'\n",
    "print(f\"Working directory: {test_folder}\")\n",
    "print(f\"Files in directory: {len(os.listdir(test_folder))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decde2f9",
   "metadata": {},
   "source": [
    "# Home Credit - Credit Risk Model Stability\n",
    "## Feature Engineering Notebook\n",
    "\n",
    "This notebook provides comprehensive feature engineering for the Home Credit Credit Risk Model Stability Kaggle project. We'll load test data, create aggregated features from multiple tables, handle missing values, and prepare engineered features for model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
